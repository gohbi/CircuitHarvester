# CircuitHarvester Technical Documentation

## Project Overview
**CircuitHarvester** is a computer vision application designed to empower makers, hobbyists, and engineers to "harvest" electronic components from old devices. By analyzing images of circuit boards, the app identifies components, provides technical specifications, suggests projects, and creates shopping lists for missing parts.

---

# Phase 1: Current Architecture (v1.0)

The current iteration allows for real-time analysis using Google's Gemini API and client-side persistence.

## 1. Tech Stack
*   **Frontend Framework**: React 18 (ESM).
*   **Styling**: Tailwind CSS (Utility-first styling for dark mode/responsive design).
*   **Icons**: Lucide React.
*   **Build/Runtime**: Browser-native ES Modules (No Webpack/Vite config required for this specific environment).

## 2. Core Services

### A. AI Analysis Service (`services/geminiService.ts`)
*   **Model**: `gemini-2.5-flash` via `@google/genai` SDK.
*   **Input**: Base64 encoded JPEG/PNG.
*   **Output Strategy**: Structured JSON generation via `responseSchema`.
*   **Key Capabilities**:
    *   Component Identification.
    *   **Spatial Grounding**: Returns `box_2d` [ymin, xmin, ymax, xmax] coordinates (0-1000 scale) for overlay rendering.
    *   Safety Warning generation based on board type.

### B. Storage Service (`services/storageService.ts`)
*   **Technology**: IndexedDB.
*   **Database**: `CircuitHarvesterDB` (Store: `sessions`).
*   **Function**: Persists the most recent scan state (image blob + analysis JSON).
*   **Retention**: 1 Hour TTL (Time To Live) to prevent stale data accumulation.

### C. Training Simulation (`services/trainingService.ts`)
*   **Role**: Acts as a stub/interface for the future backend.
*   **Current Behavior**: Creates a standardized JSON payload of the scan and simulates an upload delay.

## 3. Key Components
*   **`App.tsx`**: Main controller. Manages state transitions (Idle -> Scanning -> Analyzed).
*   **`CameraModal.tsx`**: Handles `navigator.mediaDevices.getUserMedia` video stream, draws to canvas for capture, and applies a CSS-based scanning animation.
*   **`AnnotatedImage.tsx`**: Renders the captured image and overlays interactive buttons based on the `box_2d` coordinates returned by Gemini.
*   **`AnalysisDisplay.tsx`**: Renders the analysis results, manages the "Shopping List" state, and handles Export to DOCX/Print.

---

# Phase 2: Open Source Research Agent Roadmap (v2.0)

The next phase moves from a standalone tool to a connected **AI Research Agent**. This agent will aggregate data to group circuit boards by function and generate RAG-based project suggestions.

## 1. The Core Objective

The Research Agent acts as a central brain that:
1.  **Learns**: Continuously improves by correlating scanned images with user questionnaire data.
2.  **Groups**: Clusters circuit boards visually and functionally (e.g., "This looks like a router motherboard," "This is a 90s audio amplifier").
3.  **Suggests**: Uses **RAG (Retrieval-Augmented Generation)** to look up what other users built with similar boards.

## 2. The Open Source Stack

To avoid vendor lock-in and maintain control over the research data, we will use the following state-of-the-art open-source stack:

### A. The "Eyes" (Vision Encoder)
**Model:** **LLaVA-Next (Large Language-and-Vision Assistant)** or **Qwen-VL-Chat**.
*   **Role**: Extracts semantic meaning from the circuit board images. It doesn't just "see" a capacitor; it sees "a high-power audio section."
*   **Why**: These models rival proprietary models in specific visual reasoning tasks and can be fine-tuned on custom hardware datasets.

### B. The "Brain" (Reasoning & Orchestration)
**Model:** **Meta Llama 3 (70B or 8B Instruct)**.
*   **Role**: Takes the visual data + user questionnaire and performs the logic: "Given this is a router with a powerful MIPS processor, it could be repurposed as a home automation server (Intact) or harvested for its SMA connectors (Teardown)."
*   **Serving**: Hosted via **vLLM** or **Ollama** for high-throughput inference on your own GPU server.

### C. The "Memory" (Vector Database)
**System:** **Qdrant** or **Milvus** (Open Source).
*   **Role**: Stores "Embeddings" of every scanned board.
*   **Function**: When a user uploads a board, the database performs a *Similarity Search* to find the 50 most similar boards ever scanned. This allows the system to say, "People who found this board usually built X."

## 3. Data Collection Strategy

To train this agent, we need to capture three data points from the frontend:

1.  **The Raw Image**: High-res upload.
2.  **The Automated Analysis**: The bounding boxes and labels generated by the initial scan (Phase 1).
3.  **The User Intent (Questionnaire)**:
    *   *Q1: "Do you plan to keep this device intact or tear it down?"*
    *   *Q2: "What is your skill level? (Beginner/Hobbyist/Engineer)"*
    *   *Q3: "What kind of project are you building? (Robot/IoT/Art)"*

### Database Schema (Document Store + Vector)

```json
// Collection: research_samples
{
  "_id": "uuid",
  "image_path": "s3://bucket/training/img_123.jpg",
  "user_intent": "teardown", // from Questionnaire
  "user_skill": "beginner",   // from Questionnaire
  "visual_embedding": [0.021, -0.99, ...], // 1536-dim vector from LLaVA
  "detected_cluster": "consumer_router_v2",
  "manual_labels": [ ... ] // If user corrected the AI
}
```

## 4. Implementation Roadmap

### Step 1: Backend Setup (Python/FastAPI)
Create a Python microservice that wraps the models.

```python
# pseudo_code_agent.py
from qdrant_client import QdrantClient
from llava.serve import run_model

def analyze_and_store(image, user_survey):
    # 1. Vision Analysis
    vision_features = run_model(image)
    
    # 2. Store in Vector DB
    qdrant.upsert(
        collection_name="circuit_boards",
        points=[{
            "vector": vision_features.embedding,
            "payload": { "survey": user_survey }
        }]
    )
    
    # 3. Find Similar Boards
    similar_boards = qdrant.search(
        collection_name="circuit_boards",
        query_vector=vision_features.embedding,
        limit=5
    )
    
    # 4. Generate Suggestions (Llama 3)
    suggestion = llama3.generate(
        prompt=f"User has board similar to {similar_boards}. Suggest a project."
    )
    
    return suggestion
```

### Step 2: The Questionnaire (Frontend Update)
Update the UI (`App.tsx` and `AnalysisDisplay.tsx`) to ask the user their intent *after* the scan is complete. This data becomes the "Ground Truth" for the agent.

### Step 3: The "Clustering" Job
Run a nightly job using **HDBSCAN** or **K-Means** on the Vector Database to discover new categories of devices automatically (e.g., "The system has discovered a new cluster: 'Vaping Pens'").

## 5. How to Run This Locally (Dev Environment)

To start developing the Research Agent today without a massive server:

1.  **Install Ollama**: `curl -fsSL https://ollama.com/install.sh`
2.  **Pull Models**: 
    *   `ollama run llama3`
    *   `ollama run llava`
3.  **Install Qdrant**: `docker run -p 6333:6333 qdrant/qdrant`
4.  **Connect**: Use the `langchain` library in Python to connect your uploaded images to these local models.

---

# Phase 3: Infrastructure & Containerization (Scalability)

To support scaling the application across multiple devices and cloud environments, the application has been containerized using Docker and Nginx.

## 1. Container Architecture
We utilize a **Multi-Stage Build** process to ensure the final image is lightweight and secure.

*   **Stage 1: Builder (Node.js)**
    *   Installs dependencies via `package.json`.
    *   Injects the `API_KEY` environment variable.
    *   Compiles TypeScript and React into static HTML/JS/CSS assets using **Vite**.
*   **Stage 2: Runner (Nginx)**
    *   Uses Alpine Linux for a minimal footprint (~20MB).
    *   Serves the static `dist` folder generated by the builder.
    *   Configured with `nginx.conf` to handle SPA routing (redirecting 404s to `index.html`).
    *   Implements Gzip compression for faster load times on mobile devices.

## 2. Building & Running

### Prerequisites
*   Docker installed.
*   A valid Google Gemini API Key.

### Build Command
```bash
docker build --build-arg API_KEY=your_actual_api_key_here -t circuit-harvester .
```

### Run Command
```bash
docker run -p 8080:80 circuit-harvester
```

Access the app at `http://localhost:8080`.

## 3. Kubernetes / Cloud Deployment
This container is stateless and ready for orchestration.

*   **Replicas**: Can be scaled horizontally behind a Load Balancer.
*   **Health Checks**: Nginx provides a reliable endpoint for liveness probes.
*   **Security**: The final image contains no Node.js runtime, reducing the attack surface.
